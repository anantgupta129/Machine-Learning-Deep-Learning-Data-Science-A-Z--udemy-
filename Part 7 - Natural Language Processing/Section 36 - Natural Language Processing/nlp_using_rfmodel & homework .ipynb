{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598852861431",
   "display_name": "Python 3.8.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATURAL LANGUAGE PROCESSiNG (using random forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = pd.read_csv(\"Restaurant_Reviews.tsv\", delimiter = \"\\t\", quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords        #stopwords means removing words, such as the, is, at, which etc. as they don't effect reviews \n",
    "from nltk.stem.porter import PorterStemmer      #steming means simplifying words like converting loved to love as both are +ve review\n",
    "corpus = []\n",
    "for i in range(len(dts)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dts['Review'][i])     # ^ means not, everything thats not a-z& A-Z like \"!':\" punctuations remove\n",
    "    review = review.lower()     \n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()                      # steming to optimize the dimentionality of sparse matrix\n",
    "    #all_stopwords = stopwords.words('english')\n",
    "    #all_stopwords.remove(\"not\")\n",
    "    #review =[ps.stem(word) for word in review if not word in set(all_stopwords)]    # \"\"\n",
    "    review = \" \".join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Bag of Words\n",
    "this process is also called Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "x = cv.fit_transform(corpus).toarray()\n",
    "y = dts.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting data to train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x ,y ,random_state = 42, test_size= 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data to Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=10, criterion='entropy')\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1 1]\n [1 1]\n [1 1]\n [1 1]\n [0 1]\n [1 1]\n [0 0]\n [0 1]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 0]\n [1 1]\n [0 1]\n [0 0]\n [0 0]\n [1 1]\n [0 1]\n [1 1]\n [0 0]\n [0 1]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 1]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [0 1]\n [0 0]\n [1 0]\n [0 1]\n [1 1]\n [0 0]\n [1 1]\n [0 1]\n [0 0]\n [0 0]\n [0 1]\n [0 0]\n [0 0]\n [1 0]\n [0 0]\n [1 1]\n [0 1]\n [0 1]\n [0 0]\n [0 1]\n [0 0]\n [0 0]\n [0 0]\n [0 1]\n [0 1]\n [1 1]\n [0 1]\n [0 1]\n [1 1]\n [1 1]\n [0 1]\n [0 0]\n [1 1]\n [0 1]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [1 0]\n [0 0]\n [1 1]\n [1 1]\n [0 1]\n [0 1]\n [0 1]\n [0 0]\n [0 0]\n [0 1]\n [0 1]\n [1 1]\n [0 1]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [1 0]\n [1 1]\n [1 1]\n [1 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 1]\n [0 0]\n [0 1]\n [1 1]\n [0 1]\n [1 1]\n [0 1]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 1]\n [0 0]\n [0 1]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [0 1]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [1 0]\n [1 1]\n [1 0]\n [1 1]\n [0 0]\n [0 1]\n [0 1]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [0 1]\n [1 1]\n [1 1]\n [0 0]\n [1 0]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 1]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [0 1]\n [0 0]\n [1 0]\n [0 1]\n [0 1]\n [0 0]\n [1 1]\n [1 1]\n [1 0]\n [0 0]\n [0 1]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [1 0]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [1 1]\n [0 1]\n [1 0]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 0]\n [1 1]\n [0 0]\n [1 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [1 1]\n [1 1]\n [0 1]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [1 0]\n [1 0]]\n"
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "res = np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), 1)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[111  17]\n [ 42  80]]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.764"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Challenge\n",
    "Evaluate the performance of each of these models. Try to beat the Accuracy obtained in the tutorial. But remember, Accuracy is not enough, so you should also look at other performance metrics like Precision (measuring exactness), Recall (measuring completeness) and the F1 Score (compromise between Precision and Recall). Please find below these metrics formulas (TP = # True Positives, TN = # True Negatives, FP = # False Positives, FN = # False Negatives):\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "F1 Score = 2 * Precision * Recall / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = cm.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True Positives: 80 & True Negatives: 111\nFalse Positives: 17 & False Negatives: 42\nAccuracy: 76.4%\nPrecision: 0.8247422680412371\nRecall: 0.6557377049180327\nF1 Score: 0.730593607305936\n"
    }
   ],
   "source": [
    "accu = (tn + tp) / (tn + tp + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_sc = 2 * precision * recall / (precision + recall)\n",
    "print(\"True Positives: {} & True Negatives: {}\".format(tp, tn))\n",
    "print(\"False Positives: {} & False Negatives: {}\".format(fp, fn))\n",
    "print(\"Accuracy: {}%\".format(accu*100))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "print(\"F1 Score: {}\".format(f1_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.730593607305936"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred)"
   ]
  }
 ]
}